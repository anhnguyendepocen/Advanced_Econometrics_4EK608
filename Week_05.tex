\documentclass{beamer}
%
% Choose how your presentation looks.
%
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}  
\usepackage{tikz}%boxy  
\usetikzlibrary{calc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Darmstadt}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{serif}  % or try default, serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 
%
%
\title[Week1]{Week 5: Estimators and Estimation
Methods, Nonlinear Regression, Quantile Regression}
\author{Advanced Econometrics 4EK608}
\institute{Vysoká škola ekonomická v Praze}
\date{}

\begin{document}
 
\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%---------------------------------------------
\section{Estimators and estimation methods}
\begin{frame}{Estimators and estimation methods}
Notation:
\begin{itemize}
\item $\theta$ - population parameter
\item $(x_1,x_2,\dots,x_n)$ - random sample of $n$ observation of $x$
\item $\hat{\theta}= \hat{\theta}(x_1,x_2,\dots,x_n) ~$ is an estimator of $\theta$
\end{itemize}
Basic notions: 
\begin{itemize}
\item All estimators posses sampling distribution\\
$1^{st}$ moment (mean) $\mathbf{E}(\hat{\theta})$\\
$2^{nd}$ moment (variance) $\mathbf{E}[(\hat{\theta}-\mathbf{E}(\hat{\theta}))^2]$
\item Estimators $\times$ estimate 
\item Many estimators exist for a parameter (population mean):
\end{itemize}
%\begin{block}{Example}
\begin{align*}
\hat{\theta}_1 & = \overline{x} = \frac{\sum_{i=1}^nx_i}{n}\\
\hat{\theta}_2 & = \tilde{x} = \frac{1}{2}(x_{max} +x_{min})
\end{align*}
\end{frame}


%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Small sample properties of estimators \& definitions:
\vspace{0.3cm}
\begin{itemize}
\item \textbf{Unbiasedness}: the mean of sampling distribution equals the parameter being estimated
\vspace{0.3cm}
\item \textbf{Efficiency}: an estimator is efficient if it is unbiased and no other
unbiased estimator has a smaller variance. This is usually difficult to prove, that is why we simplify the concept:
\vspace{0.3cm}
\begin{itemize}
\item Relative efficiency
\vspace{0.3cm}
\item Linear unbiased estimators instead of unbiased estimators (linear estimator is linear function of sample observations)
\end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Small sample properties of estimators \& definitions:\\
\vspace{0.5cm}
Best Linear Unbiased Estimator (BLUE) is linear, unbiased and no other linear unbiased estimator has a smaller variance. It is not necessarily the best estimator.
\vspace{0.5cm}
\begin{itemize}
\item Non-linear estimators can be better
\item Biased estimators can have smaller Mean Square Error: sum of variance and the squared bias
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Large sample properties of estimators \& definitions:
\vspace{0.3cm}
\begin{itemize}
\item Sampling distribution of an estimator changes with the size of sample. 
\vspace{0.3cm}
\item Asymptotic distribution for any estimator is that distribution to which the sampling distribution tends as the sample becomes larger. Its $1^{st}$ and $2^{nd}$ moments are asymptotic mean and asymptotic variance.
\vspace{0.3cm}
\item When the sampling distribution collapses onto a single value when the sample becomes larger, we call this value probability limit. We say estimator converges in probability to that value
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Large sample properties of estimators \& definitions:
\begin{itemize}
\item Asymptotic unbiasedness 
\bigskip
\item \textbf{Consistency} 
\item Unbiased estimators are not necessarily consistent.
\item If $\hat{\theta}$ is an unbiased estimator of $\theta$ and $\textit{var}(\hat{\theta} \rightarrow 0 \textnormal{ as } n \rightarrow \infty$, then $\textit{plim}(\hat{\theta}) = \theta$. 
\item Consistent estimators: unibased \& their variance shrinks to zero as sample size grows (entire population is used).
\begin{itemize}
\item Minimal requirement for estimator used in statistics or econometrics.
\item If some estimator is not consistent, then it does not help us with estimation of population $\theta$ values, even if we have unlimited data.
\end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Large sample properties of estimators \& definitions:
\bigskip
\begin{itemize}
\item Asymptotic efficiency: An estimator is asymptotically efficient if it is asymptotically unbiased and no other asymptotically unbiased estimator has smaller asymptotic variance.
\medskip
\item Asymptotic efficiency is usually difficult to prove, that is why we simplify the concept: 
\begin{itemize}
\medskip
\item Relative asymptotic efficiency
\smallskip
\item Linear asymptotically unbiased estimators instead of asymptotically unbiased estimators
\end{itemize}
\end{itemize}
\end{frame}
%---------------------------------------------
\subsection{Method of moments}
\begin{frame}{Estimators and estimation methods}
\textbf{Method of moments}
\medskip
\begin{itemize}
\item With the method of moments, we simply estimate population moments by corresponding sample moments. 
\item Under very general conditions, sample moments are consistent estimators of the corresponding population moments, but NOT necessarily unbiased estimators.
\end{itemize}
\begin{block}{Application example 1}
Sample covariance is a consistent estimator of population covariance.
\end{block}
\begin{block}{Application example 2}
OLS estimators we have used for parameters in the CLRM can be derived by the method of moments. 
\end{block}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\textbf{Method of moments (MM)}\\
\smallskip
\underline{Population moments}, stochastic variable $X$\\
\begin{itemize}
\item $\mathbf{E}(X^r)$: $r^{th}$ population moment about zero
\item $\mathbf{E}(X)$: the population mean is the first moment about zero
\item $\mathbf{E}[(X-\mathbf{E}(X))^2]$: the population variance is the second moment about the mean
\end{itemize}
\underline{Sample moments}, sample observations $(x_1, x_2, \dots,x_n)$
\begin{itemize}
\item $\frac{\sum_{i=1}^n x^r_i}{n}$: $r^{th}$ sample moment about zero
\item $\frac{\sum_{i=1}^n x_i}{n}=\overline{x} $ : sample mean is the first moment about zero
\item $\frac{\sum_{i=1}^n (x_i-\overline{x})^2}{n-1}$: sample variance is the second sample moment about the mean
\end{itemize}
\end{frame}

%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\begin{itemize}
\item In a LRM: $\, \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_{1} + \dots + \hat{\beta}_k x_{k} \,$, the $k+1$ parameters are \textbf{OLS}-estimated by minimizing:
\begin{equation} \label{OLS1}
\sum_{i=1}^n \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \dots - \hat{\beta}_k x_{ik} \right)^2
\end{equation}
\item $1^{st}$ order conditions for \eqref{OLS1}, plus assumptions $E(u)=0$ and $E(x_j \cdot u)=0$, can be combined into a \textbf{MM} estimator:
\begin{equation*}
\begin{aligned}
\sum_{i=1}^n ~~~~\left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \dots - \hat{\beta}_k x_{ik} \right) &= 0\\
\sum_{i=1}^n  x_{i1} \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \dots - \hat{\beta}_k x_{ik} \right) &= 0\\
\dots &\\
\sum_{i=1}^n x_{ik} \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_{i1} - \dots - \hat{\beta}_k x_{ik} \right) &= 0\\
\end{aligned}
\end{equation*}
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\textbf{MM - summary}\\
\begin{itemize}
\item MM is robust to differences in ``specification'' of the data generating process (DGP). $\rightarrow$ i.e. sample mean or sample variance estimate their population counterparts (assuming they exist) regardless of DGP.
\medskip
\item MM is free from distributional assumptions.
\medskip
\item ``Cost'' of this approach: if we know the specific distribution of a DGP, MM does not make use of such information $\rightarrow$ inefficient estimates.
\medskip 
\item Alternative approach: method of maximum likelihood utilizes distributional information and is more efficient (provided this information is valid).
\end{itemize}
\end{frame}
%---------------------------------------------
\subsection{Maximum likelihood estimation}
\begin{frame}{Estimators and Estimation Methods}
\textbf{Maximum likelihood estimator}\\
\medskip
Single $\theta$ parameter case:\\
\begin{itemize}
\item $1^{st}$ step: deriving a likelihood function $L=L(\theta ,x_1, x_2, ... , x_n)$, where $x_i$ is observation, $\theta$ is parameter of the distribution.\\
\item $2^{nd}$ step: finding maximum of $L$ with respect to $\theta$, \\that maximum is $\tilde{\theta} = \theta_{MLE}$
\end{itemize}
With more parameters: $\bm{\theta} = (\theta_1, \dots , \theta_m)^{\prime}$
$$L=L(\theta_1, \theta_2, ... \theta_m, x_1, x_2, ... , x_n)$$
We find MLEs of the $m$ parameters by partially differentiating the likelihood function $L$ with respect to each $\theta$ and then setting all the partial derivatives obtained to zero.\\
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\begin{itemize}
\item MLE is only possible if we know the form of the probability distribution function for the population.
\medskip
\item MLEs possess the large sample properties of consistency and asymptotic efficiency. There is no guarantee that they possess any desirable small-sample properties. 
\medskip
\item Under CLRM assumptions, MLE estimator are identical to OLS estimators.
\medskip
\item \textbf{Identification:} The parameter vector $\bm{\theta}$ is identified (estimable), if for two vectors, $\bm{\theta}^{*} \neq \bm{\theta}$ and for some data observations $\bm{x}$, $L(\bm{\theta}^{*}|\bm{x}) \neq L(\bm{\theta}|\bm{x})$.
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
\centerline{\underline{Maximum likelihood estimation of CLRM parameters:}}
\begin{align*}
\textnormal{CLRM: } y_i= \alpha + \beta x_i + \varepsilon_i \qquad \mathbf{E}(y_i) & =\alpha +\beta x_i \\
\textit{var}(y_i) & =\textit{var}(\varepsilon_i)=\sigma^2
\end{align*}
Probability density function for normal distribution:
$f(X)=(2\pi\sigma^2)^{-0.5} \hspace{0.1cm} exp[-(x-\mu)^2/2\sigma^2]$ \parbox[t]{10em}{where $x$ is a general \\ random variable} \\
For each $y_i$ \\
$f(y_i)=(2\pi\sigma^2)^{-0,5} \hspace{0.1cm} exp[-(y_i-\mathbf{E}(y_i))^2/2\sigma^2]$
$$L =f(y_1) \cdot f(y_2) \cdot ... \cdot f(y_n)$$
\end{frame}
%---------------------------------------------
\begin{frame}{Estimators and estimation methods}
Log-likehood function:
\begin{align*}
LL & =\sum_{i=1}^n log[f(y_i)] = \\
& = \sum_{i=1}^n \left\lbrace { -\frac{1}{2} log(2\pi)-\frac{1}{2} log(\sigma^2)-\frac{1}{2\sigma^2}[y_i-\bm{E}(y_i)]^2} \right\rbrace = \\ 
& = -\frac{n}{2}log(2\pi)-\frac{n}{2}log(\sigma^2)-\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i-\bm{E}(y_i)]^2
\end{align*}
\\ \bigskip
max $LL$ is for min $\sum_{i=1}^n[y_i-\mathbf{E}(y_i)]^2 $ \\ $\Rightarrow$ MLE estimators $\tilde{\alpha}$, $\tilde{\beta}$ are identical to OLS estimators $\hat{\alpha}$, $\hat{\beta}$\\
\end{frame}
%---------------------------------------------
\section{Nonlinear regression models}
\begin{frame}{Nonlinear regression: linear vs. nonlinear models}
\textbf{Linear model:}
\begin{align*}
y_i & = x_{i1}\beta_1 + x_{i2} \beta_2 + \dots + \varepsilon_i \\
y_i & = f_1(x_{i1}) \beta_1 + f_2(x_{i2}) \beta_2 + \dots + \varepsilon_i
\end{align*}
Conditional mean function $\mathbf{E}[y|\bm{x},\bm{\beta}]= \bm{x}'\bm{\beta}$
\\
\vspace{0.5cm}
\textbf{Nonlinear models:}\\
Linear model is a special case of the nonlinear model: 
\begin{align*}
y_i=h(x_{i1}, x_{i2}, \dots, x_{ip};\beta_1,\beta_2,\dots,\beta_K)+\varepsilon_i
\end{align*}
Conditional mean function $\mathbf{E}[y|\bm{x},\bm{\beta}]= h(\bm{x},\bm{\beta})$\\
$\partial\mathbf{E}[y|\bm{x}],\bm{\beta}]/\partial \bm{x}$ is no longer equal to $\bm\beta$, \\ then how should $\bm\beta$ be interpreted?\\
\vspace{0.2cm}
For nonlinear models, nonlinear LS have been developed.
\end{frame}
%---------------------------------------------

\begin{frame}{Nonlinear regression: linear vs. nonlinear models}
\textbf{Assumptions (comparison with the linear case)}
\begin{itemize}
\item Functional form
\item Identifiability $\times$ full rank assumption
\end{itemize}
The parameter vector in the model is identified (estimable) if there is no nonzero parameter $\bm{\beta}_0 \neq \bm{\beta}$ such that $h(\bm{x}_i, \bm{\beta}_0)=h(\bm{x}_i, \bm{\beta})$ for all $\bm{x}_i$.
\begin{itemize}
\item Zero mean of the disturbances
\item Homoscedasticity and nonautocorrelation
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Nonlinear regression: linear vs. nonlinear models}
\begin{itemize}
\item Data Generation Process
\end{itemize}
The data generating process for $\bm{x}_i$ is assumed to be a well-behaved population such that first and second moments of the data can be assumed to converge to fixed, finite population counterparts. The crucial assumption is that the process generating $\bm{x}_i$ is strictly exogenous to that generating $\varepsilon_i$
\begin{itemize}
\item Underlying probability model
\end{itemize}
There is a well-defined probability distribution generating $\varepsilon_i$. At this point, we assume only that this process produces a sample of uncorrelated, identically (marginally) distributed random variables $\varepsilon_i$ with mean zero and variance $\sigma^2$ conditioned on $h(\bm{x}_i, \bm{\beta})$. Thus, at this point, our statement of the model is \textbf{semi-parametric}.
\end{frame}
%---------------------------------------------
\begin{frame}{Nonlinear Regression: Nonlinear Least Squares}
\begin{itemize}
\item Minimization of \qquad $S(\bm{\beta})=\sum[y_i-h(\bm{x}_i, \bm{\beta})]^2$
\medskip
\end{itemize}
Using standard procedure, we can get $k$ first order conditions.
\smallskip
\begin{itemize}
\item In the context of the linear model, the \textbf{orthogonality condition} $E[\bm{x}_i, \varepsilon_i]=0$ produces least squares as a \textbf{GMM estimator} for the linear model. The orthogonality condition is that the regressors and the disturbance in the model are uncorrelated.
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Nonlinear regression: nonlinear least squares}
\begin{itemize}
\item In a similar way, the first order conditions from above are also moment conditions and this defines the nonlinear least squares estimator as a GMM estimator. This - if necessary assumptions (and some other conditions) are fulfilled - allows to deduce that the NLS estimator has good large sample properties: consistency and asymptotic normality.
\item Hypothesis testing: The principal testing procedure is the Wald test, which relies on the consistency and asymptotic normality of the estimator - large sample results. The \textit{F} test relies on normally distributed disturbances, so in the nonlinear case, where we rely on large-sample results, the Wald statistic will be the primary inference tool. \textbf{Lagrange multiplier tests} for the general case can also be constructed.
\end{itemize}
\end{frame}
%---------------------------------------------

\begin{frame}{Nonlinear regression: computing NLS estimates}
For nonlinear models, a closed-form solution usually does not exist.
\begin{itemize}
\item Most of the nonlinear maximization problems are solved by an \textbf{iterative algorithm}.
\item The most commonly used of iterative algorithms are \textbf{gradient methods}.
\item The template for most gradient methods in common use is the \textbf{Newton's method}.
\item Look at your software packages which methods are available for computing NLS estimates.
\end{itemize}
\end{frame}
%---------------------------------------------

\begin{frame}{Nonlinear regression: computing NLS estimates}
Examples 7.4 \& 7.8 (Greene):\\ Analysis of a Nonlinear Consumption Function\\
%OLS \\
NLS with starting values equal to 0 \\NLS with starting values equal to the parameters from the OLS estimation (c(3) equal to 1).
%\includegraphics[width=\textwidth, height=6cm]{Obrazek1}
\tiny
\begin{table}[]
\centering
%\caption{My caption}
%\label{my-label}
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{l}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Depednent Variable: REALCONS\\ Method: Least Squares (Marquard - EViews legacy)\\ Date: 09/19/16 Time 16:31\\ Sample 1950Q1 2000Q4\\ Included observations: 204\\ REALCONS=C(1)+C(2)*REALDPI \\
\midrule
\end{tabular}}} \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                       \\ \midrule
                                                           & Coeficient                                         & Std.Error                                         & t-Statistic                                        & Prob.                                           \\
\midrule
C(1)                                                       & -80.35475                                          & 14.30585                                          & -5.616915                                          & 0.0000                                          \\
C(2)                                                       & 0.921686                                           & 0.003872                                          & 238.0540                                           & 0.0000 
\\ \midrule
R-squared                                                  & 0.996448                                           & \multicolumn{2}{l}{Mean dependent var}                                                                 & 2999.436                                        \\
Adjusted R-squared                                         & 0.996431                                           & \multicolumn{2}{l}{S.D. dependent var}                                                                 & 1459.707                                        \\
S.E. of regression                                         & 87.20983                                           & \multicolumn{2}{l}{Akaike info criterion}                                                              & 11.78427                                        \\
Sum squared resid                                          & 1536322                                            & \multicolumn{2}{l}{Schwarz criterion}                                                                  & 11.81680                                        \\
Log likelihood                                             & -1199.995                                          & \multicolumn{2}{l}{Hannan-Quinn criter.}                                                               & 11.79743                                        \\
F-statistics                                               & 56669.72                                           & \multicolumn{2}{l}{Durbin-Watson stat}                                                                 & 0.092048                                        \\
Prob(F-statistics)                                         & 0.000000                                           & \multicolumn{2}{l}{}                                                                                   &                                                 \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------
\begin{frame}{Nonlinear regression: computing NLS estimates}
Examples 7.4 \& 7.8 (Greene): \\Analysis of a Nonlinear Consumption Function\\
%\includegraphics[width=\textwidth, height=7cm]{Obrazek2}
\tiny
\begin{table}[]
\centering
%\caption{My caption}
%\label{my-label}
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{l}{\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Depednent Variable: REALCONS\\ Method: Least Squares (Marquard - EViews legacy)\\ Sample 1950Q1 2000Q4 \quad Included observations: 204\\ Convergence achieved after 200 iterations\\ REALCONS=C(1)+C(2)*REALDPI\textasciicircum C(3) \\
\end{tabular}}} \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                            \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                            \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                            \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                            \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                            \\ \midrule
                                                             
                                                             & Coeficient                                           & Std.Error                                                    & t-Statistic                                           & Prob.                                              \\ \midrule
C(1)                                                         & 458.7991                                             & 22.50140                                                     & 20.38980                                              & 0.0000                                             \\
C(2)                                                         & 0.100852                                             & 0.010910                                                     & 9.243667                                              & 0.0000                                             \\
C(3)                                                         & 1.244827                                             & 0.012055                                                     & 103.2632                                              & 0.0000                                             \\ \midrule
R-squared                                                    & 0.998834                                             & Mean dependent var                                           &                                                       & 2999.436                                           \\
Adjusted R-squared                                           & 0.998822                                             & \multicolumn{2}{l}{S.D. dependent var}                                                                               & 1459.707                                           \\
S.E. of regression                                           & 50.09460                                             & \multicolumn{2}{l}{Akaike info criterion}                                                                            & 10.68030                                           \\
Sum squared resid                                            & 504403.2                                             & \multicolumn{2}{l}{Schwarz criterion}                                                                                & 10.72910                                           \\
Log likelihood                                               & -1086.391                                            & \multicolumn{2}{l}{Hannan-Quinn criter.}                                                                             & 10.70004                                           \\
F-statistics                                                 & 86081.29                                             & \multicolumn{2}{l}{Durbin-Watson stat}                                                                               & 0.295995                                           \\
Prob(F-statistics)                                           & 0.000000                                             & \multicolumn{2}{l}{}                                                                                                 &                                                    \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------
\begin{frame}{Nonlinear regression: computing NLS estimates}
Examples 7.4 \& 7.8 (Greene): \\Analysis of a Nonlinear Consumption Function\\
%\includegraphics[width=\textwidth, height=7cm]{Obrazek3}
\tiny
\begin{table}[]
\centering
%\caption{My caption}
%\label{my-label}
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{l}{\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Depednent Variable: REALCONS\\ Method: Least Squares (Marquard - EViews legacy)\\ Sample 1950Q1 2000Q4 \quad Included observations: 204\\ Convergence achieved after 80 iterations\\ REALCONS=C(1)+C(2)*REALDPI\textasciicircum C(3) \\ \end{tabular}}} \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                       \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                       \\ \midrule
                                                                      & Coeficient                                                    & Std.Error                                                             & t-Statistic                                                   & Prob.                                                      \\ \midrule
C(1)                                                                  & 458.7989                                                      & 22.50149                                                              & 20.38971                                                      & 0.0000                                                     \\
C(2)                                                                  & 0.100852                                                      & 0.010911                                                              & 9.243447                                                      & 0.0000                                                     \\
C(3)                                                                  & 1.244827                                                      & 0.012055                                                              & 103.2632                                                      & 0.0000                                                     \\ \midrule
R-squared                                                             & 0.998834                                                      & Mean dependent var                                                    &                                                               & 2999.436                                                   \\
Adjusted R-squared                                                    & 0.998822                                                      & \multicolumn{2}{l}{S.D. dependent var}                                                                                                & 1459.707                                                   \\
S.E. of regression                                                    & 50.09460                                                      & \multicolumn{2}{l}{Akaike info criterion}                                                                                             & 10.68030                                                   \\
Sum squared resid                                                     & 504403.2                                                      & \multicolumn{2}{l}{Schwarz criterion}                                                                                                 & 10.72910                                                   \\
Log likelihood                                                        & -1086.391                                                     & \multicolumn{2}{l}{Hannan-Quinn criter.}                                                                                              & 10.70004                                                   \\
F-statistics                                                          & 86081.28                                                      & \multicolumn{2}{l}{Durbin-Watson stat}                                                                                                & 0.295995                                                   \\
Prob(F-statistics)                                                    & 0.000000                                                      & \multicolumn{2}{l}{}                                                                                                                  &                                                            \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------
\section{Quantile regression}
\begin{frame}{Quantile regression}
\begin{itemize}
\item Quantile regression provides estimates of the relationship between regressors and a specified quantile of the dependent variable.
\item The (linear) quantile model can be defined as $Q[y|\bm{x}, q]=\bm{x\beta}$, such that $\textnormal{Prob}[y \le \bm{x\beta}_q|\bm{x}]=q$, $0<q<1$.
\item One important special case of quantile regression is the least absolute deviations (LAD) estimator, which corresponds to fitting the conditional median of the response variable ($q=\frac{1}{2}$).
\item LAD estimator can be also motivated as a robust (to outliers) alternative to LS.
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Quantile regression}
\begin{itemize}
\item The LAD estimator is the solution to the optimization problem: $\underset{\bm{\hat{\beta}}}{min} \sum|y_i-x_i\bm{\bm{\hat{\beta}}}|$
\item The LAD estimator’s history predates least squares (which itself was proposed over
200 years ago). It has seen little use in econometrics, primarily for the same reason that
Gauss’s method (LS) supplanted LAD at its origination; LS is vastly easier to compute.
\item Look at your software packages which methods are available for quantile regression.
\item It can be of some interest that the original approaches used linear programming for finding the estimate (Koenkerr and Bassett (around 1980)).
\end{itemize}
\end{frame}
%---------------------------------------------
\begin{frame}{Quantile regression}
Examples 7.9 (Greene): Cobb-Douglass Production Function\\
\quad OLS $\rightarrow$ Standardized residuals indicate two outliers $\rightarrow$ LAD
\tiny
\begin{table}[]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{l}{\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Depednent Variable: LNYN\\ Method: Least Squares\\ Sample 1 25 \\ Included observations: 25\end{tabular}}} \\
\multicolumn{5}{l}{}                                                                                                                                                                                 \\
\multicolumn{5}{l}{}                                                                                                                                                                                 \\
\multicolumn{5}{l}{}                                                                                                                                                                                 \\
\multicolumn{5}{l}{}                                                                                                                                                                                 \\
\multicolumn{5}{l}{}                                                                                                                                                                                 \\ \midrule
Variable                                     & Coeficient                           & Std.Error                          & t-Statistic                          & Prob.                              \\
\midrule
C                                            & 2.293263                             & 0.107183                           & 21.39582                             & 0.0000                             \\
LNKN                                         & 0.278982                             & 0.080686                           & 3.457639                             & 0.0022                             \\
LNLN                                         & 0.927312                             & 0.012055                           & 9.431359                             & 0.0000                             \\
\midrule
R-squared                                    & 0.959742                             & \multicolumn{2}{l}{Mean dependent var}                                    & 0.771734                           \\
Adjusted R-squared                           & 0.956082                             & \multicolumn{2}{l}{S.D. dependent var}                                    & 0.899306                           \\
S.E. of regression                           & 0.188463                             & \multicolumn{2}{l}{Akaike info criterion}                                 & -0.387663                          \\
Sum squared resid                            & 0.781403                             & \multicolumn{2}{l}{Schwarz criterion}                                     & -0.241398                          \\
Log likelihood                               & 7.845786                             & \multicolumn{2}{l}{Hannan-Quinn criter.}                                  & -0.347095                          \\
F-statistics                                 & 262.2396                             & \multicolumn{2}{l}{Durbin-Watson stat}                                    & 1.937830                           \\
Prob(F-statistics)                           & 0.000000                             & \multicolumn{2}{l}{}                                                      &                                    \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------
\begin{frame}{Quantile regression}
Examples 7.9 (Greene): Cobb-Douglass Production Function\\
\includegraphics[width=\textwidth, height=7cm]{img/Obrazek5}
\end{frame}
%---------------------------------------------
\begin{frame}{Quantile Regression}
Examples 7.9 (Greene): Cobb-Douglass Production Function\\
(results differ from the textbook results)
\tiny
\begin{table}[]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{l}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Depednent Variable: LNYN\qquad Method: Quantile Regression (Median)\\  Sample 1 25\qquad Included observations: 25\\ Huber Sandwich Standard Errors \& Covariance\\ Sparsity method: Kemel (Epanechnikov) using residuals\\ Bandwidth method: Hall-Sheather, bw=0.33227\\ Estimation successfully identifies unique optimal solution\end{tabular}}} \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                     \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                     \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                     \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                     \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                     \\ \midrule
Variable                                                                                       & Coeficient                                                                       & Std.Error                                                                       & t-Statistic                                                                       & Prob.                                                                          \\ \midrule
C                                                                                              & 2.275038                                                                         & 0.179268                                                                        & 12.69071                                                                          & 0.0000                                                                         \\
LNKN                                                                                           & 0.260365                                                                         & 0.122447                                                                        & 2.126351                                                                          & 0.0449                                                                         \\
LNLN                                                                                           & 0.927243                                                                         & 0.152593                                                                        & 6.076572                                                                          & 0.0000                                                                         \\ \midrule
Pseudo R-squared                                                                               & 0.794575                                                                         & \multicolumn{2}{l}{Mean dependent var}                                                                                                                              & 0.771734                                                                       \\
Adjusted R-squared                                                                             & 0.775900                                                                         & \multicolumn{2}{l}{S.D. dependent var}                                                                                                                              & 0.899306                                                                       \\
S.E. of regression                                                                             & 0.190505                                                                         & \multicolumn{2}{l}{Objective}                                                                                                                                       & 1.627051                                                                       \\
Quantile dependent va...                                                                       & 0.966677                                                                         & \multicolumn{2}{l}{Restr. objective}                                                                                                                                & 7.920415                                                                       \\
Sparsity                                                                                       & 0.594465                                                                         & \multicolumn{2}{l}{Quasi-LR statistic}                                                                                                                              & 84.69274                                                                       \\
Prob(Quasi-LR stat)                                                                            & 0.000000                                                                         & \multicolumn{2}{l}{}                                                                                                                                                &                                                                                \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------
\begin{frame}{Quantile regression}
Examples 7.10 (Greene): Income Elasticity of Credit Cards Expenditure\\
\quad OLS $\rightarrow$ LAD $\rightarrow$ Income Elasticity for 10 Deciles\\
\tiny
\begin{table}[]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{l}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Depednent Variable: LOGSPEND\\ Method: Least Squares\\ Date: 09/15/16 Time 13:53\\ Sample (adjusted): 3 13443\\ Included observations: 10499 after adjustments\end{tabular}}} \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                         \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                         \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                         \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                         \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                         \\ \midrule
Variable                                             & Coeficient                                   & Std.Error                                   & t-Statistic                                  & Prob.                                     \\
\midrule
C                                                    & -3.055807                                    & 0.239699                                    & -12.74852                                    & 0.0000                                    \\
LOGINC                                               & 1.083438                                     & 0.032118                                    & 33.73296                                     & 0.0000                                    \\
AGE                                                  & -0.017364                                    & 0.001348                                    & -12.88069                                    & 0.0000                                    \\
ADEPCNT                                              & -0.044610                                    & 0.010921                                    & -4.084857                                    & 0.0000                                    \\
\midrule
R-squared                                            & 0.100572                                     & \multicolumn{2}{l}{Mean dependent var}                                                     & 4.728778                                  \\
Adjusted R-squared                                   & 0.100315                                     & \multicolumn{2}{l}{S.D. dependent var}                                                     & 1.404820                                  \\
S.E. of regression                                   & 1.332496                                     & \multicolumn{2}{l}{Akaike info criterion}                                                  & 3.412366                                  \\
Sum squared resid                                    & 18634.35                                     & \multicolumn{2}{l}{Schwarz criterion}                                                      & 3.415131                                  \\
Log likelihood                                       & -17909.21                                    & \multicolumn{2}{l}{Hannah-Quinn criter.}                                                   & 3.413300                                  \\
F-statistic                                          & 391.1750                                     & \multicolumn{2}{l}{Durbin-Watson stat}                                                     & 1.888912                                  \\
Prob(F-statistic)                                    & 0.000000                                     & \multicolumn{2}{l}{}                                                                       &                                           \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------
\begin{frame}{Quantile regression}
Examples 7.10 (Greene): Income Elasticity of Credit Cards Expenditure\\
\tiny
\begin{table}[]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{l}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Depednent Variable: LOGSPEND \quad Method: Quantile Regression (Median)\\  Sample (adjusted): 3 13443\qquad Included observations: 10499 after adjustments\\ Huber Sandwich Standard Errors \& Covariance\\ Sparsity method: Kemel (Epanechnikov) using residuals\\ Bandwidth method: Hall-Sheather, bw=0.04437\\ Estimation successfully identifies unique optimal solution\end{tabular}}} \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\
\multicolumn{5}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\ \midrule
Variable                                                                                               & Coeficient                                                                               & Std.Error                                                                              & t-Statistic                                                                              & Prob.                                                                                 \\ \midrule
C                                                                                                      & -2.803756                                                                                & 0.233534                                                                               & -12.00577                                                                                & 0.0000                                                                                \\
LOGINC                                                                                                 & 1.074928                                                                                 & 0.030923                                                                               & 34.76139                                                                                 & 0.0000                                                                                \\
AGE                                                                                                    & -0.016988                                                                                & 0.001530                                                                               & -11.10597                                                                                & 0.0000                                                                                \\
ADEPCNT                                                                                                & -0.049955                                                                                & 0.011055                                                                               & -4.518599                                                                                & 0.0000                                                                                \\ \midrule
Pseudo R-squared                                                                                       & 0.058243                                                                                 & \multicolumn{2}{l}{Mean dependent var}                                                                                                                                            & 4.728778                                                                              \\
Adjusted R-squared                                                                                     & 0.057974                                                                                 & \multicolumn{2}{l}{S.D. dependent var}                                                                                                                                            & 1.404820                                                                              \\
S.E. of regression                                                                                     & 1.346476                                                                                 & \multicolumn{2}{l}{Objective}                                                                                                                                                     & 5096.818                                                                              \\
Quantile dependent va...                                                                               & 4.941583                                                                                 & \multicolumn{2}{l}{Restr. objective}                                                                                                                                              & 5412.032                                                                              \\
Sparsity                                                                                               & 2.659971                                                                                 & \multicolumn{2}{l}{Quasi-LR statistic}                                                                                                                                            & 948.0224                                                                              \\
Prob(Quasi-LR stat)                                                                                    & 0.000000                                                                                 & \multicolumn{2}{l}{}                                                                                                                                                              &                                                                                   \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}
%---------------------------------------------
\begin{frame}{Quantile regression}
Examples 7.10 (Greene): Income Elasticity of Credit Cards Expenditure\\
%\includegraphics[width=10cm, height=6.7cm]{img/Obrazek9}
\includegraphics[width=10cm, height=6.7cm]{img/quantil_regression.pdf}
\end{frame}
%---------------------------------
\end{document}